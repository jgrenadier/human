{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook is just the xgboot_predictor.ipynb from\n",
    "# https://github.com/FergusOBoyle/sustainable-dev-goals-forecasting\n",
    "# and I have added...\n",
    "# 1. code to write out intermediate results to csv files.\n",
    "# 2. added an algortihm derived from running the csv data through \n",
    "#    Microsoft's AutoML that automatically generated a more  optimized\n",
    "#    model.  The code generated by AutoML needing some tweaking when run\n",
    "#    outside of their Azure cloud but did\n",
    "#    generate a better RMSE score notetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics.scorer import make_scorer\n",
    "import xgboost as xgb \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import preprocess, missing, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307 indicators included\n"
     ]
    }
   ],
   "source": [
    "target = 'SI.POV.DDAY'\n",
    "predict_year=2010\n",
    "#percent of input Indicators to use (set to 100 for full set of input features)\n",
    "percent = 50\n",
    "\n",
    "#Load the data from disk\n",
    "input_dir = '.\\\\..\\\\data\\\\'\n",
    "data_input = \"cleaned_data.pkl\"\n",
    "data = pd.read_pickle(input_dir + data_input)\n",
    "\n",
    "#Possible subset of data choosen to reduce calulation time\n",
    "#For percetages less than 100% we try to choose a subset that represents the spread of variables\n",
    "\n",
    "if percent == 100:\n",
    "    pass\n",
    "else: \n",
    "    num_indicators_original = data.shape[1]\n",
    "    step = int(100/percent)\n",
    "    data_new = data.iloc[:,::step].copy()\n",
    "    #Add the target column if not already included\n",
    "    if target not in data_new.columns:\n",
    "        data_new[target] = data[target]\n",
    "    data = data_new\n",
    "    \n",
    "print(data.shape[1], \"indicators included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jagNote: write out some intermediate data for inspection\n",
    "\n",
    "# save pk. data back to a csv (for use in Azure)\n",
    "\n",
    "import pickle as pkl\n",
    "# import pandas as pd\n",
    "\n",
    "input_dir2 = '.\\\\..\\\\data\\\\'\n",
    "data_input2 = \"cleaned_data.pkl\"\n",
    "\n",
    "with open(input_dir2 + data_input2, \"rb\") as f2:\n",
    "    object = pkl.load(f2)\n",
    "    \n",
    "df2 = pd.DataFrame(object)\n",
    "df2.to_csv(input_dir2 + r'cleaned_data2.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%time data_regressors, data_targets = \\\n",
    "        preprocess.window_data(data, lag=3,num_windows=10, step=1, predict_year=2010, \\\n",
    "                         target=target, impute_type='interpolation')\n",
    "\n",
    "#Break up into training and testing data.\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "data_train_regressors = data_regressors.loc[idx[:,2:10],:]\n",
    "data_train_targets = data_targets.loc[idx[:,2:10],:]\n",
    "data_test_regressors = data_regressors.loc[idx[:,1],:]\n",
    "data_test_targets= data_targets.loc[idx[:,1],:]\n",
    "\n",
    "#For Training, only consider windows that don't have a missing target as they offer nothing to training\n",
    "#Therefore, remove those observations from both the training regressors and targets datasets.\n",
    "data_train_regressors_subset = data_train_regressors[~np.isnan(list(data_train_targets.values.flatten()))]\n",
    "data_train_targets_subset = data_train_targets[~np.isnan(list(data_train_targets.values.flatten()))]\n",
    "\n",
    "#For testing, also remove windows with no target variable as it is impossible to measure preformance.\n",
    "data_test_regressors_subset = data_test_regressors[~np.isnan(list(data_test_targets.values.flatten()))]\n",
    "data_test_targets_subset = data_test_targets[~np.isnan(list(data_test_targets.values.flatten()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train_regressors_subset.values\n",
    "y_train = data_train_targets_subset.values.ravel()\n",
    "X_test = data_test_regressors_subset.values\n",
    "y_test = data_test_targets_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# jagNote: write out some intermediate data for inspection...\n",
    "\n",
    "print(type(data_train_regressors_subset))\n",
    "print(data_train_regressors_subset.shape)\n",
    "print(data_train_targets_subset.shape)\n",
    "print(data_test_regressors_subset.shape)\n",
    "print(data_test_targets_subset.shape)\n",
    "\n",
    "# \n",
    "# combine the dataframes vertically\n",
    "xxx = pd.concat([data_train_regressors_subset,data_test_regressors_subset])\n",
    "print(xxx.shape)\n",
    "\n",
    "yyy = pd.concat([data_train_targets_subset,data_test_targets_subset])\n",
    "print(yyy.shape)\n",
    "\n",
    "# combine horizontally (to add target back to a combined dataframe)\n",
    "xxxyyy = pd.concat([xxx, yyy.set_index(xxx.index)], axis=1)\n",
    "\n",
    "# write out to a csv file\n",
    "\n",
    "input_dir5 = '.\\\\..\\\\data\\\\'\n",
    "    \n",
    "xxxyyy.to_csv(input_dir5 + r'combined_data5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# jagNote: Let's save tabular data that has some preprocessing and perhaps\n",
    "# feature engineering back to csv files so we can use this data up in the\n",
    "# microsoft azure cloud using AutoML...\n",
    "# note: we'll upload a combined training and test csv to azure and allow\n",
    "# automl to do their own splitting into training and test datasets.\n",
    "\n",
    "\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print(type(X_test))\n",
    "print(type(y_test))\n",
    "\n",
    "# print(X_train.size)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "#\n",
    "# combine train and test arrrays\n",
    "#\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "print(X_combined.shape)\n",
    "y_train3 = y_train.reshape((455,1))\n",
    "y_combined = np.vstack((y_train3, y_test))\n",
    "print(y_combined.shape)\n",
    "\n",
    "xy_combined = np.concatenate((X_combined, y_combined),axis=1)\n",
    "\n",
    "\n",
    "input_dir3 = '.\\\\..\\\\data\\\\'\n",
    "\n",
    "#\n",
    "# write numpy array back to csv\n",
    "\n",
    "np.savetxt(input_dir3 + r'combined_data3.csv', xy_combined, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-the-box using the Scikit-learn interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of XGBoost out-of-the-box is: 5.767635272547369\n"
     ]
    }
   ],
   "source": [
    "XGB = xgb.XGBRegressor(random_state=42 ,objective='reg:squarederror', subsample=0.9)\n",
    "XGB.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = XGB.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"RMSE of XGBoost out-of-the-box is:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning of the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "cv_folds = 5\n",
    "\n",
    "scorer = make_scorer(mean_squared_error ,greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Tune the number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of estimators: 100\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(random_state=42,\n",
    "                         objective='reg:squarederror',\n",
    "                         max_depth=5, \n",
    "                         min_child_weight = 1, \n",
    "                         gamma = 0, \n",
    "                         subsample=0.9, \n",
    "                         colsample_bytree = 0.8, \n",
    "                         scale_pos_weight = 1)\n",
    "\n",
    "param = model.get_xgb_params()\n",
    "data_matrix = xgb.DMatrix(X_train, label=y_train)\n",
    "cvresult = xgb.cv(param, data_matrix, num_boost_round=model.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=50)\n",
    "#Set the optimised number of estimators\n",
    "model.set_params(n_estimators=cvresult.shape[0])\n",
    "print(\"Optimal number of estimators:\", cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of xgboost after tuning (step 1) is: 5.513444558601978\n"
     ]
    }
   ],
   "source": [
    "model.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = model.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"RMSE of xgboost after tuning (step 1) is:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Tune max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of xgboost after tuning (step 2) is: 5.39043909814746\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "#grid_model = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "#                        n_jobs=4,iid=False, cv=5)\n",
    "grid_model = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "                        n_jobs=4,cv=5)\n",
    "grid_model.fit(X_train,y_train)\n",
    "\n",
    "#Score the best model using the test data\n",
    "model = grid_model.best_estimator_\n",
    "model.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = model.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"RMSE of xgboost after tuning (step 2) is:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of our test shows that the performance of the model after tuning was actually worse than before. The model is generalising very poorly. This may be a reflection on some kind of bias introduced in creating our training and data subsets. It would be worth looking at how I decided to discard any countries early on that did not have target values for the target year, 2010. It may have made more sense to window the data and split into training and test subsets and then, after, discard any observations that did not have a target value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_child_weight': 5}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Tune Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of xgboost after tuning (step 3) is: 5.394199542667251\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#grid_model = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "#                        n_jobs=4,iid=False, cv=5)\n",
    "grid_model = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "                        n_jobs=4,cv=5)\n",
    "grid_model.fit(X_train,y_train)\n",
    "\n",
    "#Score the best model using the test data\n",
    "model = grid_model.best_estimator_\n",
    "model.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = model.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"RMSE of xgboost after tuning (step 3) is:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Tune Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of xgboost after tuning (step 4) is: 5.394199570122429\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1]\n",
    "}\n",
    "\n",
    "#grid_model = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "#                        n_jobs=4,iid=False, cv=5 ,return_train_score=True)\n",
    "grid_model = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "                        n_jobs=4,cv=5 ,return_train_score=True)\n",
    "grid_model.fit(X_train,y_train)\n",
    "\n",
    "#Score the best model using the test data\n",
    "model = grid_model.best_estimator_\n",
    "model.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = model.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"RMSE of xgboost after tuning (step 4) is:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_alpha': 1e-05}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of xgboost after tuning (step 5) is: 5.513699313865063\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    " 'reg_lambda':[1e-5, 1e-2, 0.1, 1]\n",
    "}\n",
    "\n",
    "#grid_model = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "#                        n_jobs=4,iid=False, cv=5 ,return_train_score=True)\n",
    "grid_model = GridSearchCV(model, param_grid = params, scoring=scorer,\n",
    "                        n_jobs=4,cv=5 ,return_train_score=True)\n",
    "grid_model.fit(X_train,y_train)\n",
    "\n",
    "#Score the best model using the test data\n",
    "model = grid_model.best_estimator_\n",
    "model.fit( X_train,y_train)\n",
    "#Make predictions\n",
    "predictions = model.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"RMSE of xgboost after tuning (step 5) is:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model with no Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 605 ms\n"
     ]
    }
   ],
   "source": [
    "%time data_regressors, data_targets = \\\n",
    "        preprocess.window_data(data, lag=3,num_windows=10, step=1, predict_year=2010, \\\n",
    "                         target=target)\n",
    "\n",
    "#Break up into training and testing data.\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "data_train_regressors = data_regressors.loc[idx[:,2:10],:]\n",
    "data_train_targets = data_targets.loc[idx[:,2:10],:]\n",
    "data_test_regressors = data_regressors.loc[idx[:,1],:]\n",
    "data_test_targets= data_targets.loc[idx[:,1],:]\n",
    "\n",
    "#For Training, only consider windows that don't have a missing target as they offer nothing to training\n",
    "#Therefore, remove those observations from both the training regressors and targets datasets.\n",
    "data_train_regressors_subset = data_train_regressors[~np.isnan(list(data_train_targets.values.flatten()))]\n",
    "data_train_targets_subset = data_train_targets[~np.isnan(list(data_train_targets.values.flatten()))]\n",
    "\n",
    "#For testing, also remove windows with no target variable as it is impossible to measure preformance.\n",
    "data_test_regressors_subset = data_test_regressors[~np.isnan(list(data_test_targets.values.flatten()))]\n",
    "data_test_targets_subset = data_test_targets[~np.isnan(list(data_test_targets.values.flatten()))]\n",
    "\n",
    "X_train_miss = data_train_regressors_subset.values\n",
    "y_train_miss = data_train_targets_subset.values.ravel()\n",
    "X_test_miss = data_test_regressors_subset.values\n",
    "y_test_miss = data_test_targets_subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-the-box xgboost on data without imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand-tuning of the XGBoost model. I pick some important paramneters and play around until I get a good result. I'm sure there is more accuracy that can be obtained from this model by gridsearching but I think this is enough to illustrate that using a XGBoost (or perhaps any tree-based predictive algo) without any imputation done on the input data gives by far the best results of any of the models tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of XGBoost out-of-the-box is: 5.231545740351754\n"
     ]
    }
   ],
   "source": [
    "XGB = xgb.XGBRegressor(n_estimators=200,  objective='reg:squarederror',max_depth=7, subsample=0.87, reg_lambda=0.2)\n",
    "XGB.fit( X_train_miss,y_train_miss)\n",
    "#Make predictions\n",
    "predictions = XGB.predict(X_test_miss) \n",
    "\n",
    "mse= mean_squared_error(y_test_miss, predictions)\n",
    "print(\"RMSE of XGBoost out-of-the-box is:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jagNote:  Below is code that was not in xgboot_predictor.ipynb.\n",
    "# Instead this code was derived automatically by AutooML and then\n",
    "# modified to run on my local PC in my Anaconda environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Microsoft ModelBuilder script.py\n",
    "import logging\n",
    "logger = logging.getLogger(\"azureml.training.tabular\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Microsoft ModelBuilder script.py\n",
    "def generate_preprocessor_config_0():\n",
    "    from sklearn.preprocessing import MaxAbsScaler\n",
    "    \n",
    "    preproc = MaxAbsScaler(\n",
    "        copy=True\n",
    "    )\n",
    "    \n",
    "    return preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Microsoft ModelBuilder script.py\n",
    "def generate_algorithm_config_0():\n",
    "    # from lightgbm.sklearn import LGBMRegressor\n",
    "    import lightgbm as lgb\n",
    "    algorithm = lgb.LGBMRegressor(\n",
    "        boosting_type='gbdt',\n",
    "        class_weight=None,\n",
    "        colsample_bytree=1.0,\n",
    "        importance_type='split',\n",
    "        learning_rate=0.1,\n",
    "        max_depth=-1,\n",
    "        min_child_samples=20,\n",
    "        min_child_weight=0.001,\n",
    "        min_split_gain=0.0,\n",
    "        n_estimators=100,\n",
    "        n_jobs=-1,\n",
    "        num_leaves=31,\n",
    "        objective=None,\n",
    "        random_state=None,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=0.0,\n",
    "        silent=True,\n",
    "        subsample=1.0,\n",
    "        subsample_for_bin=200000,\n",
    "        subsample_freq=0,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    return algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_algorithm_config_1():\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    \n",
    "    algorithm = ExtraTreesRegressor(\n",
    "        bootstrap=False,\n",
    "        ccp_alpha=0.0,\n",
    "        criterion='mse',\n",
    "        max_depth=None,\n",
    "        max_features=0.5,\n",
    "        max_leaf_nodes=None,\n",
    "        max_samples=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        min_impurity_split=None,\n",
    "        min_samples_leaf=0.005080937188890647,\n",
    "        min_samples_split=0.0012814223889440828,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        n_estimators=50,\n",
    "        n_jobs=-1,\n",
    "        oob_score=False,\n",
    "        random_state=None,\n",
    "        verbose=0,\n",
    "        warm_start=False\n",
    "    )\n",
    "    \n",
    "    return algorithm\n",
    "\n",
    "def generate_algorithm_config_1():\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    \n",
    "    algorithm = ExtraTreesRegressor(\n",
    "        bootstrap=False,\n",
    "        ccp_alpha=0.0,\n",
    "        criterion='mse',\n",
    "        max_depth=None,\n",
    "        max_features=0.5,\n",
    "        max_leaf_nodes=None,\n",
    "        max_samples=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        min_samples_leaf=0.005080937188890647,\n",
    "        min_samples_split=0.0012814223889440828,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        n_estimators=50,\n",
    "        n_jobs=-1,\n",
    "        oob_score=False,\n",
    "        random_state=None,\n",
    "        verbose=0,\n",
    "        warm_start=False\n",
    "    )\n",
    "    \n",
    "    return algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Microsoft ModelBuilder script.py\n",
    "def generate_preprocessor_config_1():\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    preproc = MinMaxScaler(\n",
    "        copy=True,\n",
    "        feature_range=(0, 1)\n",
    "    )\n",
    "    \n",
    "    return preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preprocessor_config_2():\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    preproc = MinMaxScaler(\n",
    "        copy=True,\n",
    "        feature_range=(0, 1)\n",
    "    )\n",
    "    \n",
    "    return preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Microsoft ModelBuilder script.py\n",
    "def generate_algorithm_config_2():\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    \n",
    "    algorithm = ElasticNet(\n",
    "        alpha=0.001,\n",
    "        copy_X=True,\n",
    "        fit_intercept=True,\n",
    "        l1_ratio=0.8436842105263158,\n",
    "        max_iter=1000,\n",
    "        normalize=False,\n",
    "        positive=False,\n",
    "        precompute=False,\n",
    "        random_state=None,\n",
    "        selection='cyclic',\n",
    "        tol=0.0001,\n",
    "        warm_start=False\n",
    "    )\n",
    "    \n",
    "    return algorithm\n",
    "\n",
    "\n",
    "from sklearn.linear_model import ElasticNet    \n",
    "algorithm = ElasticNet(\n",
    "    alpha=0.001,\n",
    "    copy_X=True,\n",
    "    fit_intercept=True,\n",
    "    l1_ratio=0.8436842105263158,\n",
    "    max_iter=1000,\n",
    "    normalize=False,\n",
    "    positive=False,\n",
    "    precompute=False,\n",
    "    random_state=None,\n",
    "    selection='cyclic',\n",
    "    tol=0.0001,\n",
    "    warm_start=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Microsoft ModelBuilder script.py\n",
    "def generate_algorithm_config():\n",
    "    # from azureml.automl.runtime.shared.model_wrappers import PreFittedSoftVotingRegressor\n",
    "    # from sklearn.ensemble import VotingRegressor\n",
    "    from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    pipeline_0 = Pipeline(steps=[('preproc', generate_preprocessor_config_0()), ('model', generate_algorithm_config_0())])\n",
    "    pipeline_1 = Pipeline(steps=[('preproc', generate_preprocessor_config_1()), ('model', generate_algorithm_config_1())])\n",
    "    pipeline_2 = Pipeline(steps=[('preproc', generate_preprocessor_config_2()), ('model', generate_algorithm_config_2())])\n",
    "    #algorithm = PreFittedSoftVotingRegressor(\n",
    "    algorithm = VotingRegressor(\n",
    "        estimators=[\n",
    "            ('model_0', pipeline_0),\n",
    "            ('model_1', pipeline_1),\n",
    "            ('model_2', pipeline_2),\n",
    "        ],\n",
    "        weights=[0.5, 0.42857142857142855, 0.07142857142857142]\n",
    "    )\n",
    "    \n",
    "    return algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Microsoft ModelBuilder script.py\n",
    "def build_model_pipeline():\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    logger.info(\"Running build_model_pipeline\")\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('featurization', generate_data_transformation_config()),\n",
    "            ('ensemble', generate_algorithm_config()),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Microsoft ModelBuilder script.py\n",
    "def train_model(X, y, sample_weights):\n",
    "    # logger.info(\"Running train_model\")\n",
    "    model_pipeline = build_model_pipeline()\n",
    "    \n",
    "    model = model_pipeline.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Microsoft ModelBuilder script.py\n",
    "def calculate_metrics(model, X, y, sample_weights, X_test, y_test, cv_splits=None):\n",
    "    from azureml.training.tabular.preprocessing.binning import make_dataset_bins\n",
    "    from azureml.training.tabular.score.scoring import score_regression\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_min = np.min(y)\n",
    "    y_max = np.max(y)\n",
    "    y_std = np.std(y)\n",
    "    \n",
    "    bin_info = make_dataset_bins(X_test.shape[0], y_test)\n",
    "    metrics = score_regression(\n",
    "        y_test, y_pred, ['normalized_root_mean_squared_error'], y_max, y_min, y_std, sample_weights, bin_info)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_transformation_config():\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "    \n",
    "    column_group_1 = [['Column1'], ['Column2'], ['Column3'], ['Column4'], ['Column5'], ['Column6'], ['Column7'], ['Column8'], ['Column9'], ['Column10'], ['Column11'], ['Column12'], ['Column13'], ['Column14'], ['Column15'], ['Column16'], ['Column17'], ['Column18'], ['Column19'], ['Column20'], ['Column21'], ['Column22'], ['Column23'], ['Column24'], ['Column25'], ['Column26'], ['Column27'], ['Column28'], ['Column29'], ['Column30'], ['Column31'], ['Column32'], ['Column33'], ['Column34'], ['Column35'], ['Column36'], ['Column37'], ['Column38'], ['Column39'], ['Column40'], ['Column41'], ['Column42'], ['Column43'], ['Column44'], ['Column45'], ['Column46'], ['Column47'], ['Column48'], ['Column49'], ['Column50'], ['Column51'], ['Column52'], ['Column53'], ['Column54'], ['Column55'], ['Column56'], ['Column57'], ['Column58'], ['Column59'], ['Column60'], ['Column61'], ['Column62'], ['Column63'], ['Column64'], ['Column65'], ['Column66'], ['Column67'], ['Column68'], ['Column69'], ['Column70'], ['Column71'], ['Column72'], ['Column73'], ['Column74'], ['Column75'], ['Column76'], ['Column77'], ['Column78'], ['Column79'], ['Column80'], ['Column81'], ['Column82'], ['Column83'], ['Column84'], ['Column85'], ['Column86'], ['Column87'], ['Column88'], ['Column89'], ['Column90'], ['Column91'], ['Column92'], ['Column93'], ['Column94'], ['Column95'], ['Column96'], ['Column97'], ['Column98'], ['Column99'], ['Column100'], ['Column101'], ['Column102'], ['Column103'], ['Column104'], ['Column105'], ['Column106'], ['Column107'], ['Column108'], ['Column109'], ['Column110'], ['Column111'], ['Column112'], ['Column113'], ['Column114'], ['Column115'], ['Column116'], ['Column117'], ['Column118'], ['Column119'], ['Column120'], ['Column121'], ['Column122'], ['Column123'], ['Column124'], ['Column125'], ['Column126'], ['Column127'], ['Column128'], ['Column129'], ['Column130'], ['Column131'], ['Column132'], ['Column133'], ['Column134'], ['Column135'], ['Column136'], ['Column137'], ['Column138'], ['Column139'], ['Column140'], ['Column141'], ['Column142'], ['Column143'], ['Column144'], ['Column145'], ['Column146'], ['Column147'], ['Column148'], ['Column149'], ['Column150'], ['Column151'], ['Column152'], ['Column153'], ['Column154'], ['Column155'], ['Column156'], ['Column157'], ['Column158'], ['Column159'], ['Column160'], ['Column161'], ['Column162'], ['Column163'], ['Column164'], ['Column165'], ['Column166'], ['Column167'], ['Column168'], ['Column169'], ['Column170'], ['Column171'], ['Column172'], ['Column173'], ['Column174'], ['Column175'], ['Column176'], ['Column177'], ['Column178'], ['Column179'], ['Column180'], ['Column181'], ['Column182'], ['Column183'], ['Column184'], ['Column185'], ['Column186'], ['Column187'], ['Column188'], ['Column189'], ['Column190'], ['Column191'], ['Column192'], ['Column193'], ['Column194'], ['Column195'], ['Column196'], ['Column197'], ['Column198'], ['Column199'], ['Column200'], ['Column201'], ['Column202'], ['Column203'], ['Column204'], ['Column205'], ['Column206'], ['Column207'], ['Column208'], ['Column209'], ['Column210'], ['Column211'], ['Column212'], ['Column213'], ['Column214'], ['Column215'], ['Column216'], ['Column217'], ['Column218'], ['Column219'], ['Column220'], ['Column221'], ['Column222'], ['Column223'], ['Column224'], ['Column225'], ['Column226'], ['Column227'], ['Column228'], ['Column229'], ['Column230'], ['Column231'], ['Column232'], ['Column233'], ['Column234'], ['Column235'], ['Column236'], ['Column237'], ['Column238'], ['Column239'], ['Column240'], ['Column241'], ['Column242'], ['Column243'], ['Column244'], ['Column245'], ['Column246'], ['Column247'], ['Column248'], ['Column249'], ['Column250'], ['Column251'], ['Column252'], ['Column253'], ['Column254'], ['Column255'], ['Column256'], ['Column257'], ['Column258'], ['Column259'], ['Column260'], ['Column261'], ['Column262'], ['Column263'], ['Column264'], ['Column265'], ['Column266'], ['Column267'], ['Column268'], ['Column269'], ['Column270'], ['Column271'], ['Column272'], ['Column273'], ['Column274'], ['Column275'], ['Column276'], ['Column277'], ['Column278'], ['Column279'], ['Column280'], ['Column281'], ['Column282'], ['Column283'], ['Column284'], ['Column285'], ['Column286'], ['Column287'], ['Column288'], ['Column289'], ['Column290'], ['Column291'], ['Column292'], ['Column293'], ['Column294'], ['Column295'], ['Column296'], ['Column297'], ['Column298'], ['Column299'], ['Column300'], ['Column301'], ['Column302'], ['Column303'], ['Column304'], ['Column305'], ['Column306'], ['Column307'], ['Column308'], ['Column309'], ['Column310'], ['Column311'], ['Column312'], ['Column313'], ['Column314'], ['Column315'], ['Column316'], ['Column317'], ['Column318'], ['Column319'], ['Column320'], ['Column321'], ['Column322'], ['Column323'], ['Column324'], ['Column325'], ['Column326'], ['Column327'], ['Column328'], ['Column329'], ['Column330'], ['Column331'], ['Column332'], ['Column333'], ['Column334'], ['Column335'], ['Column336'], ['Column337'], ['Column338'], ['Column339'], ['Column340'], ['Column341'], ['Column342'], ['Column343'], ['Column344'], ['Column345'], ['Column346'], ['Column347'], ['Column348'], ['Column349'], ['Column350'], ['Column351'], ['Column352'], ['Column353'], ['Column354'], ['Column355'], ['Column356'], ['Column357'], ['Column358'], ['Column359'], ['Column360'], ['Column361'], ['Column362'], ['Column363'], ['Column364'], ['Column365'], ['Column366'], ['Column367'], ['Column368'], ['Column369'], ['Column370'], ['Column371'], ['Column372'], ['Column373'], ['Column374'], ['Column375'], ['Column376'], ['Column377'], ['Column378'], ['Column379'], ['Column380'], ['Column381'], ['Column382'], ['Column383'], ['Column384'], ['Column385'], ['Column386'], ['Column387'], ['Column388'], ['Column389'], ['Column390'], ['Column391'], ['Column392'], ['Column393'], ['Column394'], ['Column395'], ['Column396'], ['Column397'], ['Column398'], ['Column399'], ['Column400'], ['Column401'], ['Column402'], ['Column403'], ['Column404'], ['Column405'], ['Column406'], ['Column407'], ['Column408'], ['Column409'], ['Column410'], ['Column411'], ['Column412'], ['Column413'], ['Column414'], ['Column415'], ['Column416'], ['Column417'], ['Column418'], ['Column419'], ['Column420'], ['Column421'], ['Column422'], ['Column423'], ['Column424'], ['Column425'], ['Column426'], ['Column427'], ['Column428'], ['Column429'], ['Column430'], ['Column431'], ['Column432'], ['Column433'], ['Column434'], ['Column435'], ['Column436'], ['Column437'], ['Column438'], ['Column439'], ['Column440'], ['Column441'], ['Column442'], ['Column443'], ['Column444'], ['Column445'], ['Column446'], ['Column447'], ['Column448'], ['Column449'], ['Column450'], ['Column451'], ['Column452'], ['Column453'], ['Column454'], ['Column455'], ['Column456'], ['Column457'], ['Column458'], ['Column459'], ['Column460'], ['Column461'], ['Column462'], ['Column463'], ['Column464'], ['Column465'], ['Column466'], ['Column467'], ['Column468'], ['Column469'], ['Column470'], ['Column471'], ['Column472'], ['Column473'], ['Column474'], ['Column475'], ['Column476'], ['Column477'], ['Column478'], ['Column479'], ['Column480'], ['Column481'], ['Column482'], ['Column483'], ['Column484'], ['Column485'], ['Column486'], ['Column487'], ['Column488'], ['Column489'], ['Column490'], ['Column491'], ['Column492'], ['Column493'], ['Column494'], ['Column495'], ['Column496'], ['Column497'], ['Column498'], ['Column499'], ['Column500'], ['Column501'], ['Column502'], ['Column503'], ['Column504'], ['Column505'], ['Column506'], ['Column507'], ['Column508'], ['Column509'], ['Column510'], ['Column511'], ['Column512'], ['Column513'], ['Column514'], ['Column515'], ['Column516'], ['Column517'], ['Column518'], ['Column519'], ['Column520'], ['Column521'], ['Column522'], ['Column523'], ['Column524'], ['Column525'], ['Column526'], ['Column527'], ['Column528'], ['Column529'], ['Column530'], ['Column531'], ['Column532'], ['Column533'], ['Column534'], ['Column535'], ['Column536'], ['Column537'], ['Column538'], ['Column539'], ['Column540'], ['Column541'], ['Column542'], ['Column543'], ['Column544'], ['Column545'], ['Column546'], ['Column547'], ['Column548'], ['Column549'], ['Column550'], ['Column551'], ['Column552'], ['Column553'], ['Column554'], ['Column555'], ['Column556'], ['Column557'], ['Column558'], ['Column559'], ['Column560'], ['Column561'], ['Column562'], ['Column563'], ['Column564'], ['Column565'], ['Column566'], ['Column567'], ['Column568'], ['Column569'], ['Column570'], ['Column571'], ['Column572'], ['Column573'], ['Column574'], ['Column575'], ['Column576'], ['Column577'], ['Column578'], ['Column579'], ['Column580'], ['Column581'], ['Column582'], ['Column583'], ['Column584'], ['Column585'], ['Column586'], ['Column587'], ['Column588'], ['Column589'], ['Column590'], ['Column591'], ['Column592'], ['Column593'], ['Column594'], ['Column595'], ['Column596'], ['Column597'], ['Column598'], ['Column599'], ['Column600'], ['Column601'], ['Column602'], ['Column603'], ['Column604'], ['Column605'], ['Column606'], ['Column607'], ['Column608'], ['Column609'], ['Column610'], ['Column611'], ['Column612'], ['Column613'], ['Column614'], ['Column615'], ['Column616'], ['Column617'], ['Column618'], ['Column619'], ['Column620'], ['Column621'], ['Column622'], ['Column623'], ['Column624'], ['Column625'], ['Column626'], ['Column627'], ['Column628'], ['Column629'], ['Column630'], ['Column631'], ['Column632'], ['Column633'], ['Column634'], ['Column635'], ['Column636'], ['Column637'], ['Column638'], ['Column639'], ['Column640'], ['Column641'], ['Column642'], ['Column643'], ['Column644'], ['Column645'], ['Column646'], ['Column647'], ['Column648'], ['Column649'], ['Column650'], ['Column651'], ['Column652'], ['Column653'], ['Column654'], ['Column655'], ['Column656'], ['Column657'], ['Column658'], ['Column659'], ['Column660'], ['Column661'], ['Column662'], ['Column663'], ['Column664'], ['Column665'], ['Column666'], ['Column667'], ['Column668'], ['Column669'], ['Column670'], ['Column671'], ['Column672'], ['Column673'], ['Column674'], ['Column675'], ['Column676'], ['Column677'], ['Column678'], ['Column679'], ['Column680'], ['Column681'], ['Column682'], ['Column683'], ['Column684'], ['Column685'], ['Column686'], ['Column687'], ['Column688'], ['Column689'], ['Column690'], ['Column691'], ['Column692'], ['Column693'], ['Column694'], ['Column695'], ['Column696'], ['Column697'], ['Column698'], ['Column699'], ['Column700'], ['Column701'], ['Column702'], ['Column703'], ['Column704'], ['Column705'], ['Column706'], ['Column707'], ['Column708'], ['Column709'], ['Column710'], ['Column711'], ['Column712'], ['Column713'], ['Column714'], ['Column715'], ['Column716'], ['Column717'], ['Column718'], ['Column719'], ['Column720'], ['Column721'], ['Column722'], ['Column723'], ['Column724'], ['Column725'], ['Column726'], ['Column727'], ['Column728'], ['Column729'], ['Column730'], ['Column731'], ['Column732'], ['Column733'], ['Column734'], ['Column735'], ['Column736'], ['Column737'], ['Column738'], ['Column739'], ['Column740'], ['Column741'], ['Column742'], ['Column743'], ['Column744'], ['Column745'], ['Column746'], ['Column747'], ['Column748'], ['Column749'], ['Column750'], ['Column751'], ['Column752'], ['Column753'], ['Column754'], ['Column755'], ['Column756'], ['Column757'], ['Column758'], ['Column759'], ['Column760'], ['Column761'], ['Column762'], ['Column763'], ['Column764'], ['Column765'], ['Column766'], ['Column767'], ['Column768'], ['Column769'], ['Column770'], ['Column771'], ['Column772'], ['Column773'], ['Column774'], ['Column775'], ['Column776'], ['Column777'], ['Column778'], ['Column779'], ['Column780'], ['Column781'], ['Column782'], ['Column783'], ['Column784'], ['Column785'], ['Column786'], ['Column787'], ['Column788'], ['Column789'], ['Column790'], ['Column791'], ['Column792'], ['Column793'], ['Column794'], ['Column795'], ['Column796'], ['Column797'], ['Column798'], ['Column799'], ['Column800'], ['Column801'], ['Column802'], ['Column803'], ['Column804'], ['Column805'], ['Column806'], ['Column807'], ['Column808'], ['Column809'], ['Column810'], ['Column811'], ['Column812'], ['Column813'], ['Column814'], ['Column815'], ['Column816'], ['Column817'], ['Column818'], ['Column819'], ['Column820'], ['Column821'], ['Column822'], ['Column823'], ['Column824'], ['Column825'], ['Column826'], ['Column827'], ['Column828'], ['Column829'], ['Column830'], ['Column831'], ['Column832'], ['Column833'], ['Column834'], ['Column835'], ['Column836'], ['Column837'], ['Column838'], ['Column839'], ['Column840'], ['Column841'], ['Column842'], ['Column843'], ['Column844'], ['Column845'], ['Column846'], ['Column847'], ['Column848'], ['Column849'], ['Column850'], ['Column851'], ['Column852'], ['Column853'], ['Column854'], ['Column855'], ['Column856'], ['Column857'], ['Column858'], ['Column859'], ['Column860'], ['Column861'], ['Column862'], ['Column863'], ['Column864'], ['Column865'], ['Column866'], ['Column867'], ['Column868'], ['Column869'], ['Column870'], ['Column871'], ['Column872'], ['Column873'], ['Column874'], ['Column875'], ['Column876'], ['Column877'], ['Column878'], ['Column879'], ['Column880'], ['Column881'], ['Column882'], ['Column883'], ['Column884'], ['Column885'], ['Column886'], ['Column887'], ['Column888'], ['Column889'], ['Column890'], ['Column891'], ['Column892'], ['Column893'], ['Column894'], ['Column895'], ['Column896'], ['Column897'], ['Column898'], ['Column899'], ['Column900'], ['Column901'], ['Column902'], ['Column903'], ['Column904'], ['Column905'], ['Column906'], ['Column907'], ['Column908'], ['Column909'], ['Column910'], ['Column911'], ['Column912'], ['Column913'], ['Column914'], ['Column915'], ['Column916'], ['Column917'], ['Column918'], ['Column919'], ['Column920'], ['Column921']]\n",
    "    \n",
    "    mapper = get_mapper_ab1045(column_group_1)\n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapper_ab1045(column_names):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn_pandas.dataframe_mapper import DataFrameMapper\n",
    "    from sklearn_pandas.features_generator import gen_features\n",
    "    \n",
    "    definition = gen_features(\n",
    "        columns=column_names,\n",
    "        classes=[\n",
    "            {\n",
    "                'class': SimpleImputer,\n",
    "                'add_indicator': False,\n",
    "                'copy': True,\n",
    "                'fill_value': None,\n",
    "                'missing_values': np.nan,\n",
    "                'strategy': 'mean',\n",
    "                'verbose': 0,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    mapper = DataFrameMapper(features=definition, input_df=True, sparse=True)\n",
    "    \n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgren\\anaconda3\\envs\\my_new_envz1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:400: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      "  FutureWarning,\n",
      "C:\\Users\\jgren\\anaconda3\\envs\\my_new_envz1\\lib\\site-packages\\sklearn\\linear_model\\_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  FutureWarning,\n",
      "C:\\Users\\jgren\\anaconda3\\envs\\my_new_envz1\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.537e+02, tolerance: 5.403e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is: 4.761559497612451\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# need to convert to use microsoft azure algorithm\n",
    "\n",
    "# XGB = xgb.XGBRegressor(n_estimators=200,  objective='reg:squarederror',max_depth=7, subsample=0.87, reg_lambda=0.2)\n",
    "# XGB.fit( X_train_miss,y_train_miss)\n",
    "# #Make predictions\n",
    "# predictions = XGB.predict(X_test_miss) \n",
    "\n",
    "# mse= mean_squared_error(y_test_miss, predictions)\n",
    "# print(\"RMSE of XGBoost out-of-the-box is:\", np.sqrt(mse))\n",
    "\n",
    "myalgol = generate_algorithm_config()\n",
    "myalgol.fit( X_train,y_train)\n",
    "\n",
    "predictions = myalgol.predict(X_test) \n",
    "\n",
    "mse= mean_squared_error(y_test, predictions)\n",
    "print(\"RMSE is:\", np.sqrt(mse))\n",
    "\n",
    "# jagNote: Some warning are generated below but the code runs ok for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
